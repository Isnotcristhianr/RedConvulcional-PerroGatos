{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isnotcristhianr/RedConvulcional-PerroGatos/blob/main/RedConvucional_PrediccionImgs_PerroGato.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1HshwCgWKdU"
      },
      "source": [
        "# Redes Neuronales de Convolución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lidXLdDfzoLA"
      },
      "source": [
        "## Instalación de Theano\n",
        "#### pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
        "\n",
        "## Instalación Tensorflow\n",
        "#### pip install tensorflow\n",
        "\n",
        "## Instalación de Keras\n",
        "#### pip install --upgrade keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_zYxNUAzuXE"
      },
      "source": [
        "# Parte 1 - Construir la CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXgeoJp_zzuP"
      },
      "source": [
        "## Importación de las librerías y paquetes de Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pchrP86cWTrQ"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEiwlq4G3Go9"
      },
      "source": [
        "## Inicializar la CNN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9zmSZFo3xMS"
      },
      "source": [
        "classifier = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dvde9Cs3ygC"
      },
      "source": [
        "## Paso 1 - Convolución"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGMgPL0i39Zz"
      },
      "source": [
        "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12PVk3Jg3_eU"
      },
      "source": [
        "## Paso 2 - Agrupar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0YoKBog4Dn8"
      },
      "source": [
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXDG767u4KC4"
      },
      "source": [
        "## Agregar una segunda capa convolucional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmtpliu84LSw"
      },
      "source": [
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG1OwcrI5RaI"
      },
      "source": [
        "## Paso 3- Flattening\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfE0cQg15T2w"
      },
      "source": [
        "classifier.add(Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuOR4S76xFOh"
      },
      "source": [
        "## Paso 4 - Capa fully connected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiKdfPlE5bvg"
      },
      "source": [
        "classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYkUgF2X5d0O"
      },
      "source": [
        "## Compilar la CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z2iN3fd5f-o"
      },
      "source": [
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rg4jrPq5iAj"
      },
      "source": [
        "#Parte 2 - Ajustar la CNN a las imágenes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IBBkjIvcJbk",
        "outputId": "ea111643-9de2-4a15-fe12-fb20f9e03d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3_Dm_fJ8hiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c07973-f56f-49f8-8a37-1d7b0fdffd54"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_data_dir = '/content/drive/My Drive/training_set'\n",
        "test_data_dir = '/content/drive/My Drive/test_set'\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(train_data_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(test_data_dir,\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')\n",
        "\n",
        "# Obtener el número total de muestras en los conjuntos de entrenamiento y validación\n",
        "train_samples = training_set.n\n",
        "val_samples = test_set.n\n",
        "\n",
        "# Calcular los pasos por época para el entrenamiento y la validación\n",
        "train_steps_per_epoch = train_samples // training_set.batch_size\n",
        "val_steps_per_epoch = val_samples // test_set.batch_size\n",
        "\n",
        "# Asumiendo que classifier es tu modelo ya definido y compilado\n",
        "classifier.fit(training_set,\n",
        "               steps_per_epoch=train_steps_per_epoch,\n",
        "               epochs=25,\n",
        "               validation_data=test_set,\n",
        "               validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8048 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "251/251 [==============================] - 188s 748ms/step - loss: 0.6176 - accuracy: 0.6659 - val_loss: 0.5594 - val_accuracy: 0.7117\n",
            "Epoch 2/25\n",
            "251/251 [==============================] - 105s 417ms/step - loss: 0.5763 - accuracy: 0.6960 - val_loss: 0.5199 - val_accuracy: 0.7465\n",
            "Epoch 3/25\n",
            "251/251 [==============================] - 109s 434ms/step - loss: 0.5427 - accuracy: 0.7253 - val_loss: 0.5227 - val_accuracy: 0.7490\n",
            "Epoch 4/25\n",
            "251/251 [==============================] - 107s 424ms/step - loss: 0.5251 - accuracy: 0.7386 - val_loss: 0.4964 - val_accuracy: 0.7581\n",
            "Epoch 5/25\n",
            "251/251 [==============================] - 98s 389ms/step - loss: 0.4917 - accuracy: 0.7584 - val_loss: 0.5071 - val_accuracy: 0.7520\n",
            "Epoch 6/25\n",
            "251/251 [==============================] - 106s 423ms/step - loss: 0.4851 - accuracy: 0.7621 - val_loss: 0.4921 - val_accuracy: 0.7535\n",
            "Epoch 7/25\n",
            "251/251 [==============================] - 107s 426ms/step - loss: 0.4644 - accuracy: 0.7776 - val_loss: 0.4890 - val_accuracy: 0.7641\n",
            "Epoch 8/25\n",
            "251/251 [==============================] - 106s 422ms/step - loss: 0.4421 - accuracy: 0.7882 - val_loss: 0.4695 - val_accuracy: 0.7797\n",
            "Epoch 9/25\n",
            "251/251 [==============================] - 105s 417ms/step - loss: 0.4333 - accuracy: 0.7934 - val_loss: 0.4628 - val_accuracy: 0.7737\n",
            "Epoch 10/25\n",
            "251/251 [==============================] - 105s 417ms/step - loss: 0.4133 - accuracy: 0.8036 - val_loss: 0.4721 - val_accuracy: 0.7908\n",
            "Epoch 11/25\n",
            "251/251 [==============================] - 105s 420ms/step - loss: 0.4017 - accuracy: 0.8127 - val_loss: 0.4484 - val_accuracy: 0.7928\n",
            "Epoch 12/25\n",
            "251/251 [==============================] - 106s 420ms/step - loss: 0.3862 - accuracy: 0.8237 - val_loss: 0.4750 - val_accuracy: 0.7883\n",
            "Epoch 13/25\n",
            "251/251 [==============================] - 97s 385ms/step - loss: 0.3755 - accuracy: 0.8285 - val_loss: 0.4434 - val_accuracy: 0.7918\n",
            "Epoch 14/25\n",
            "251/251 [==============================] - 99s 393ms/step - loss: 0.3634 - accuracy: 0.8386 - val_loss: 0.4707 - val_accuracy: 0.7923\n",
            "Epoch 15/25\n",
            "251/251 [==============================] - 104s 416ms/step - loss: 0.3522 - accuracy: 0.8433 - val_loss: 0.4680 - val_accuracy: 0.7954\n",
            "Epoch 16/25\n",
            "251/251 [==============================] - 105s 419ms/step - loss: 0.3246 - accuracy: 0.8575 - val_loss: 0.4510 - val_accuracy: 0.7989\n",
            "Epoch 17/25\n",
            "251/251 [==============================] - 97s 386ms/step - loss: 0.3212 - accuracy: 0.8613 - val_loss: 0.4775 - val_accuracy: 0.7979\n",
            "Epoch 18/25\n",
            "251/251 [==============================] - 105s 417ms/step - loss: 0.2967 - accuracy: 0.8740 - val_loss: 0.5168 - val_accuracy: 0.7944\n",
            "Epoch 19/25\n",
            "251/251 [==============================] - 104s 413ms/step - loss: 0.2930 - accuracy: 0.8764 - val_loss: 0.4982 - val_accuracy: 0.7873\n",
            "Epoch 20/25\n",
            "251/251 [==============================] - 98s 389ms/step - loss: 0.2822 - accuracy: 0.8754 - val_loss: 0.5024 - val_accuracy: 0.8075\n",
            "Epoch 21/25\n",
            "251/251 [==============================] - 107s 426ms/step - loss: 0.2638 - accuracy: 0.8837 - val_loss: 0.5301 - val_accuracy: 0.7979\n",
            "Epoch 22/25\n",
            "251/251 [==============================] - 97s 386ms/step - loss: 0.2560 - accuracy: 0.8916 - val_loss: 0.4943 - val_accuracy: 0.8085\n",
            "Epoch 23/25\n",
            "251/251 [==============================] - 97s 386ms/step - loss: 0.2401 - accuracy: 0.9026 - val_loss: 0.5300 - val_accuracy: 0.8004\n",
            "Epoch 24/25\n",
            "251/251 [==============================] - 105s 420ms/step - loss: 0.2161 - accuracy: 0.9091 - val_loss: 0.5969 - val_accuracy: 0.7949\n",
            "Epoch 25/25\n",
            "251/251 [==============================] - 106s 423ms/step - loss: 0.2054 - accuracy: 0.9175 - val_loss: 0.5712 - val_accuracy: 0.7939\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e3e427630a0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}